---
title: "More Random Effects"
output: html_notebook
editor_options: 
  chunk_output_type: inline
---


```{r chunk_setup, include=FALSE, eval=TRUE}
knitr::opts_chunk$set(echo = T, message=F, warning=F, comment=NA, autodep=F, 
                      eval=T, cache.rebuild=F, cache=T, R.options=list(width=120), 
                      fig.width=8, fig.align = 'center', dev.args=list(bg = 'transparent'), dev='svglite')
```

```{r catchup}
# if needed
library(tidyverse)
library(lme4)

load('data/gpa.RData')

gpa_lm = lm(gpa ~ occasion, data=gpa)
```

## Application

Add a random slope to our previous example and examine the results.

```{r random_slope}
gpa_mixed =  lmer(gpa ~ occasion + (1 + occasion|student), data=gpa)
summary(gpa_mixed)
```

### Explore Random Effects

As before, we can examine the per-student random effects. 

```{r random_effects}
ranef(gpa_mixed)$student
```

Unlike before, we see each student's occasion effect.

```{r random_coefficients}
coef(gpa_mixed)$student
```


## Comparison to Many Regressions

The following code calculates regression models for each student (i.e. six observations apiece). Density plots are shown comparing regressions run for each student and the mixed model.

```{r by_group}
gpa_lm_by_group0 = lmList(gpa ~ occasion | student, gpa)
gpa_lm_by_group  = coef(gpa_lm_by_group0)

gint = 
  data_frame(Mixed=coef(gpa_mixed)$student[,1], Separate=gpa_lm_by_group[,1]) %>% 
  gather(key=Model, value=Intercept) %>% 
  ggplot(aes(x=Intercept)) +
  geom_density(aes(color=Model, fill=Model), alpha=.25) +
  scale_color_viridis_d(begin = .25, end = .75) +
  scale_fill_viridis_d(begin = .25, end = .75) +
  ggtitle('Intercepts') +
  labs(x='', y='') +
  xlim(c(1.5,4)) +
  theme_minimal() +
  theme(
    axis.text.y = element_blank(),
    axis.ticks.y = element_blank(),
    legend.key.size=unit(2, 'mm'),
    legend.title=element_text(size=8),
    legend.text=element_text(size=8),
    legend.box.spacing=unit(0, 'in'),
    legend.position=c(.85,.75)
    )

gslopes = 
  data_frame(Mixed=coef(gpa_mixed)$student[,2], Separate=gpa_lm_by_group[,2]) %>% 
  gather(key=Model, value=Occasion) %>% 
  ggplot(aes(x=Occasion)) +
  geom_density(aes(color=Model, fill=Model), alpha=.25, show.legend=F) +
  scale_color_viridis_d(begin = .25, end = .75) +
  scale_fill_viridis_d(begin = .25, end = .75) +
  ggtitle('Slopes for occasion') +
  labs(x='', y='') +
  xlim(c(-.2,.4)) +
  theme_minimal() +
  theme(
    axis.text.y = element_blank(),
    axis.ticks.y = element_blank()
  )


library(patchwork)
gint + gslopes
```

## Visualization of Effects


Let's look at what the results are in terms of prediction. First we can look at the mixed effects results.

```{r visualize_mixed_fit}
# add if you want, not displayed as there are only a couple negative slopes
going_down = factor(rep(coef(gpa_mixed)$student[,'occasion']<0, e=6), labels=c('Up', 'Down'))

library(modelr) # allows us to add predictions to the data frame
gpa %>% 
  add_predictions(gpa_lm, var='lm') %>% 
  add_predictions(gpa_mixed, var='mixed') %>% 
  ggplot() + 
  geom_line(aes(x=occasion, y=mixed, group=student), alpha=.1, color='#00aaff') +
  geom_line(aes(x=occasion, y=lm, group=student), color='#ff5500') +
  labs(y='gpa') + 
  theme_minimal() +
  theme(panel.grid.minor = element_blank(),
        panel.grid.major.x = element_blank())
```

With that in mind, now we can see the messier 'by-group' approach.  The general trend is ignored, and many more students are predicted with downward trends when they probably shouldn't be.

```{r visualize_by_group_fit}
going_down = factor(rep(gpa_lm_by_group[,'occasion']<0, e=6), labels=c('Upward', 'Downward'))

gpa %>% 
  mutate(stufit=fitted(gpa_lm_by_group0)) %>% 
  add_predictions(gpa_lm, var='gpa') %>% 
  add_predictions(gpa_lm, var='lm') %>% 
  ggplot() +
  geom_line(aes(x=occasion, y=stufit, group=student, color=going_down, alpha = going_down)) +
  geom_line(aes(x=occasion, y=lm), 
            color='#ff5500') +
  labs(y='gpa') + 
  theme_minimal() +
  theme(panel.grid.minor = element_blank(),
        panel.grid.major.x = element_blank())
```



## Exercises

#### Sleep revisited

Run the sleep study model with random coefficient for the Days effect, and interpret the results.  What is the correlation between the intercept and Days random effects?  Use the `ranef` and `coef` functions on the model you've created to inspect the individual specific effects. What do you see?

```{r sleepstudy2, eval=FALSE}
library(lme4)
data("sleepstudy")

model = lmer(?, data = sleepstudy)

summary(model)
```

In the following, run each line, inspecting the result of each as you go along. 

```{r, eval=FALSE}
re = ranef(model)$Subject
fe = fixef(model)
apply(re, 1, function(x) x + fe) %>% t
```

The above code adds the fixed effects to each row of the random effects (the `t` just transposes the result). What is the result compared to what you saw before?



#### Simulation revisited

The following shows a simplified way to simulate some random slopes, but otherwise is the same as the simulation before.  Go ahead and run the code.

```{r simSlopes}
set.seed(1234)  # this will allow you to exactly duplicate your result
Ngroups = 50
NperGroup = 3
N = Ngroups * NperGroup
groups = factor(rep(1:Ngroups, each = NperGroup))
re_int = rnorm(Ngroups, sd = .75)
re_slope = rnorm(Ngroups, sd = .25)
e = rnorm(N, sd = .25)
x = rnorm(N)
y = (2 + re_int[groups]) + (.5 + re_slope[groups]) * x + e

d = data.frame(x, y, groups)
```

This next bit of code shows a way to run a mixed model while specifying that there is no correlation between intercepts and slopes.  There is generally no reason to do this unless the study design warrants it, but you could do it as a step in the model-building process, such that you fit a model with no correlation, then one with it.

```{r simSlopes2, eval=FALSE}
model_ints_only = lmer(y ~ x + (1 | groups), data = d)
model_with_slopes = lmer(y ~ x + (1 | groups) + (0 + x | groups), data = d)
summary(model_with_slopes)
confint(model_with_slopes)

library(ggplot2)
ggplot(aes(x, y), data=d) +
  geom_point()
```

Compare model fit using the `AIC` function, e.g. `AIC(model)`.  The model with the lower AIC is the better model, so which would you choose?
