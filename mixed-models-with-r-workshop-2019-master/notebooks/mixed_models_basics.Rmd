---
title: "Mixed Models: Basics"
output: html_notebook
editor_options: 
  chunk_output_type: inline
---


```{r chunk_setup, include=FALSE, eval=TRUE}
knitr::opts_chunk$set(echo = T, message=F, warning=F, comment=NA, autodep=F, 
                      eval=T, cache.rebuild=F, cache=T, R.options=list(width=120), 
                      fig.width=8, fig.align = 'center', dev.args=list(bg = 'transparent'), dev='svglite')
```


## Example: Student GPA

Load up and inspect the data.

```{r basic_packages}
library(tidyverse)
```


```{r load_gpa_data}
load('data/gpa.RData')
gpa
```

## Application

### Standard Regression

We'll start with a standard linear regression model. We have coefficients for the intercept and the effect of time, and in addition, the variance of the observations (residual standard error).

```{r gpa_lm}
gpa_lm = lm(gpa ~ occasion, data=gpa)
summary(gpa_lm)
```

### Mixed Model

```{r gpa_mixed}
library(lme4)
gpa_mixed = lmer(gpa ~ occasion + (1|student), data=gpa)
summary(gpa_mixed)
```

[As a test, replace `1|student` with `1|sample(1:10, 1200, replace = T)`.  As your variance due to arbitrary grouping is essentially 0, the residual error estimate is similar to the `lm` model.]


People always ask where the p-values are, but the answer is... complicated.  Other packages and programs present them as if they are trivially obtained, but that is not the case, and the `lme4` developers would rather not make unnecessary assumptions.  On the plus side, you can get interval estimates easily enough, even though they are poorly named for the variance components. `sigma01` is the student variance.

```{r gpa_mixed_confint}
confint(gpa_mixed)
```

#### Estimated Random Effects

Now examine the random effects.

```{r gpa_mixed_ranef}
ranef(gpa_mixed)$student 
```

```{r gpa_mixed_rancoef}
coef(gpa_mixed)$student 
```

As we didn't allow the occasion effect to vary, it is constant.  We'll change this later.

#### Prediction

```{r gpa_mixed_prediction}
predict(gpa_mixed, re.form=NA) %>% head
```

## Adding a Cluster-level Covariate

See exercises.



## Exercises


### Sleep

For this exercise, we'll use the sleep study data from the `lme4` package.  The following describes it.

> The average reaction time per day for subjects in a sleep deprivation study. On day 0 the subjects had their normal amount of sleep. Starting that night they were restricted to 3 hours of sleep per night. The observations represent the average reaction time (in milliseconds) on a series of tests given each day to each subject.

After loading the package, the data can be loaded as follows.  I show the first few observations.

```{r sleepstudy}
library(lme4)
data("sleepstudy")
head(sleepstudy)
```

1. Run a regression with Reaction as the target variable and Days as the predictor. 

2. Run a mixed model with a random intercept for Subject.

3. Interpret the variance components and fixed effects.

4. What would a plot of the prediction lines per student look like relative to the overall trend?



### Cluster level covariate

Rerun the mixed model with the GPA data adding the cluster level covariate of `sex`, or high school GPA (`highgpa`), or both.  Interpret all aspects of the results.

```{r gpa_cluster, eval=FALSE}
gpa_mixed_cluster_level = lmer(?, gpa)

summary(gpa_mixed_cluster_level)
```

What happened to the student variance after adding cluster level covariates to the model?



### Simulation

The following represents a simple way to simulate a random intercepts model.  Note each object what each object is, and make sure the code make sense to you.  Then run it.

```{r simMixed}
set.seed(1234)  # this will allow you to exactly duplicate your result
Ngroups = 100
NperGroup = 3
N = Ngroups * NperGroup
groups = factor(rep(1:Ngroups, each = NperGroup))
u = rnorm(Ngroups, sd = .5)
e = rnorm(N, sd = .25)
x = rnorm(N)
y = 2 + .5 * x + u[groups] + e

d = data.frame(x, y, groups)
```

Which of the above represent the fixed and random effects? Now run the following.

```{r simMixed2}
model = lmer(y ~ x + (1|groups), data=d)
summary(model)
confint(model)



library(ggplot2)
ggplot(aes(x, y), data=d) +
  geom_point()
```

Do the results seem in keeping with what you expect?

In what follows we'll change various aspects of the data, then rerun the model after each change, then summarize and get confidence intervals as before.  For each note specifically at least one thing that changed in the results.

0. First calculate or simply eyeball the intraclass correlation coefficient:

$$\frac{\textrm{random effect variance}}{\textrm{residual + random effect variance}}$$

In addition, create a density plot of the random effects as follows.

```{r simMixed3, eval=FALSE}
re = ranef(model)$groups
qplot(x=re, geom='density', xlim=c(-3,3))
```

1. Change the random effect variance/sd and/or the residual variance/sd and note your new estimate of the ICC, and plot the random effect as before.

2. Reset the values to the original.  Change <span class="objclass">Ngroups</span> to 50. What differences do you see in the confidence interval estimates? 

3. Set the Ngroups back to 100. Now change <span class="objclass">NperGroup</span> to 10, and note again the how the CI is different from the base condition.