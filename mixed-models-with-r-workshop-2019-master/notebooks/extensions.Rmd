---
title: "Common Extensions"
output: html_notebook
editor_options: 
  chunk_output_type: inline
---


```{r chunk_setup, include=FALSE, eval=TRUE}
knitr::opts_chunk$set(echo = T, message=F, warning=F, comment=NA, autodep=F, 
                      eval=T, cache.rebuild=F, cache=T, R.options=list(width=120), 
                      fig.width=8, fig.align = 'center', dev.args=list(bg = 'transparent'), dev='svglite')
```

```{r catchup}
# if needed
library(tidyverse)
library(lme4)

load('data/gpa.RData')

# if you want to run all the code, you'll need to install the following
# install.packages(c('sjstats', 'merTools', 'broom'))
```


## Additional Grouping Structure

### Crossed random effects

Load the pupils data. We'll look at achievement scores for students.  The sources of dependency are due to students having gone to the same primary or secondary schools.  However, in this example, going to a primary school doesn't necessarily mean you'll go to a specific secondary school.  Note also that there are no repeated measures, we see each student only once.

```{r load_pupils_data}
load('data/pupils.RData')

pupils
```

#### Primary and Secondary School Random Effects

We'll do a model with random effects for both primary and secondary school.  As such, we'll get variance components for each as well.

```{r cross_classified}
pupils_crossed = lmer(achievement ~ sex + ses 
                      + (1|primary_school_id) + (1|secondary_school_id), 
                      data = pupils)

summary(pupils_crossed, correlation=F)
```

In this case, the primary school displays more variance.

```{r crossed_vc}
VarCorr(pupils_crossed) %>% 
  print(comp=c('Var', 'Std'), digits=3)
```

```{r crossed_icc}
sjstats::icc(pupils_crossed) # relative proportion due to each school
```

Note the specific random effects for 50 distinct primary schools vs. 30 distinct secondary schools.

```{r crossed_re}
str(ranef(pupils_crossed))
```

We can use the `merTools` package to visualize.  Install it if you haven't already.  This plot allows to visually see the increased variability due to primary schools relative to secondary.

```{r crossed_re_plot}
library(merTools)

plotREsim(REsim(pupils_crossed)) +
  theme_minimal()
```


### Hierarchical Structure

Load and inspect the nurses data. Here we are interested in the effect of a training program (`treatment`) on stress levels (on a scale of 1-7) of nurses.  In this scenario, nurses are nested within wards, which themselves are nested within hospitals, so we will have random effects pertaining to ward (within hospital) and hospital.

```{r nurses_data, echo=1}
load('data/nurses.RData')

nurses
```

There are two different ways to note a nested structure with `lme4`.  Either is fine.  As before, we have two sources of variability, and with this data it is with ward and hospital.

```{r hierarchical}
nurses_hierarchical = lmer(stress ~ age + sex + experience 
                           + treatment + wardtype + hospsize 
                           + (1|hospital) + (1|hospital:ward), 
                           data = nurses)

nurses_hierarchical = lmer(stress ~ age  + sex + experience 
                           + treatment + wardtype + hospsize 
                           + (1|hospital/ward), 
                           data = nurses) # same thing!

summary(nurses_hierarchical, correlation=F)
```

```{r hierarchical_fixed}
nurses_hierarchical %>% 
  broom::tidy('fixed', conf.int=T) %>% 
  mutate_if(is.numeric, round, digits = 2)
```


As far as the fixed effects go, about the only thing that doesn't have a statistical effect is ward type.  

There appears to be more variability due to ward than that due to hospital.

```{r hierarchical_random}
VarCorr(nurses_hierarchical)
```



### Crossed vs. Nested

The following shows the difference in the results from treating ward as a nested (within hospital) vs. crossed random effect. What do you notice is different?

```{r crossed_vs_nested, message=F}
nurses_hierarchical1 = lmer(stress ~ age  + sex + experience 
                           + treatment + wardtype + hospsize 
                           + (1|hospital) + (1|hospital:ward), data = nurses)

nurses_crossed1 = lmer(stress ~ age  + sex + experience 
                           + treatment + wardtype + hospsize 
                           + (1|hospital) + (1|ward), data = nurses)

nurses_crossed2 = lmer(stress ~ age  + sex + experience 
                           + treatment + wardtype + hospsize 
                           + (1|hospital) + (1|wardid), data = nurses)
```


```{r nested1}
VarCorr(nurses_hierarchical1)
```

```{r crossed1}
VarCorr(nurses_crossed1)
```

```{r crossed2}
VarCorr(nurses_crossed2)
```


The first hierarchical model and the second crossed version are identical. The second crossed is incorrectly labeled to be using a crossed notation for the model, as the label doesn't distinguish ward 1 in hospital 1 from ward 1 in hospital 2.  In the second crossed model, we use `wardid` instead of `ward`, so each ward has a unique id, and the proper structure is accounted for with  the crossed syntax.


## Residual Structure


### Heterogeneous Variances

```{r heterovar}
library(nlme)
heterovar_res = lme(gpa ~ occasion, 
                    data = gpa,
                    random = ~1|student, 
                    weights = varIdent(form = ~1|occasion))

summary(heterovar_res)
```

```{r relative_variances}
# values are relative to redisual variance and on the standard deviation scale
summary(heterovar_res$modelStruct)
```

```{r heterovar_glmmTMB}
library(glmmTMB)
heterovar_res2 = glmmTMB(gpa ~ occasion 
                         + (1|student) + diag(0 + occas |student), 
                         data = gpa)

summary(heterovar_res2) # you can ignore the Corr part in the random effects output
```

```{r glmmtmb_extract_variances}
vc_glmmtmb = VarCorr(heterovar_res2)
vc_glmmtmb = attr(vc_glmmtmb$cond$student.1, 'stddev')^2 + sigma(heterovar_res2)^2
vc_glmmtmb
```

### Autocorrelation

```{r corr_residual}
corr_res = lme(
  gpa ~ occasion, 
  data = gpa,
  random = ~1|student, 
  correlation = corAR1(form = ~occasion)
)

corr_res
```

## Generalized Linear Mixed Models

Note that `nlme` does not model beyond the gaussian distribution, so we go back to using `lme4` and the `glmer` function.  You may notice that it takes the model a second or two even though it is not that complex.  GLMM are often hard to estimate, and you will often encounter convergence issues.  Scaling the data can help a lot.

```{r glmm_speed_dating}
load('data/speed_dating.RData')

sd_model = glmer(
  decision ~ sex + samerace + attractive_sc + sincere_sc + intelligent_sc 
  + (1|iid), 
  data = speed_dating,
  family = binomial
)

summary(sd_model, correlation=F)
```

Note that the participant effect (`iid`) is almost as large (in terms of the standard deviation) as the effect of attractiveness.


## Exercises


### Sociometric data

In the following data, kids are put into different groups and rate each other in terms of how much they would like to share some activity with the others. We have identifying variables for the person doing the rating (sender), the person being rated (receiver), what group they are in, as well as age and sex for both sender and receiver, as well as group size.

To run a mixed model, we will have three sources of structure to consider:

- senders (within group)
- receivers (within group)
- group

First, load the sociometric data. 

```{r load_socio}
load('data/sociometric.RData')
```


To run the model, we will proceed with the following modeling steps. For each, make sure you are creating a separate model object for each model run.

- Model 1: No covariates, only sender and receiver random effects. Note that even though we don't add group yet, still use the nesting approach to specify the effects (e.g. `1|group:receiver`)
- Model 2: No covariates, add group random effect
- Model 3: Add all covariates: `agesend/rec`, `sexsend/rec`, and `grsize` (group size)
- Model 4: In order to examine sex matching effects, add an interaction of the sex variables to the model `sexsend:sexrec`.
- Compare models with AIC, e.g. `AIC(model1)`. A lower value would indicate the model is preferred.


```{r socio}
model1 = lmer(rating ~ (1|group:sender) + (1|group:receiver), 
             data=sociometric)
summary(model1, correlation=F) 

model2 = lmer(rating ~ (1|group:sender) + (1|group:receiver) + (1|group), 
             data=sociometric)
summary(model2, correlation=F) 

model3 = lmer(rating ~ sexsend + sexrec + agesend + agerec + grsize 
              + (1|group:sender) + (1|group:receiver) + (1|group), 
             data=sociometric)
summary(model3, correlation=F)

model4 = lmer(rating ~ sexsend*sexrec + agesend + agerec + grsize 
              + (1|group:sender) + (1|group:receiver) + (1|group), 
             data=sociometric)
summary(model4, correlation=F)

c(AIC(model1), AIC(model2), AIC(model3), AIC(model4))
```




### Patents

Do a Poisson mixed effect model using the `patent` data.  Model the number of citations (`ncit`) based on whether there was opposition to the patent (`opposition`) and if it was for the biotechnology/pharmaceutical industry (`biopharm`). Use year as a random effect to account for unspecified economic conditions.  


```{r patent_starter}
load('data/patents.RData')

model_poisson = glmer(ncit ~ opposition + biopharm + (1 | year), 
                      data = patents, 
                      family = 'poisson')

summary(model_poisson)
```


Interestingly, one can model overdispersion in a Poisson model by specifying an random intercept for each observation (`subject` in the data).  In other words, no clustering or grouped structure is necessary.
